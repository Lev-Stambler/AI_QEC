{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ECCT' already exists and is not an empty directory.\n",
      "ECCT\t     codey_pylib.py\t\tquick_start_pytorch_images\n",
      "README.md    model_out\t\t\ttest.txt\n",
      "__pycache__  quick_start_pytorch.ipynb\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yoniLc/ECCT.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We will use ECC_Transfomer as a mechanism for decoding. On top of the model though, we wil add an initial \"array shaping\" layer. I think we can use a ?transformer? there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 49])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Yoni Choukroun, choukroun.yoni@gmail.com\n",
    "Error Correction Code Transformer\n",
    "https://arxiv.org/abs/2203.14966\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def Read_pc_matrixrix_alist(fileName):\n",
    "    with open(fileName, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        columnNum, rowNum = np.fromstring(\n",
    "            lines[0].rstrip('\\n'), dtype=int, sep=' ')\n",
    "        H = np.zeros((rowNum, columnNum)).astype(int)\n",
    "        for column in range(4, 4 + columnNum):\n",
    "            nonZeroEntries = np.fromstring(\n",
    "                lines[column].rstrip('\\n'), dtype=int, sep=' ')\n",
    "            for row in nonZeroEntries:\n",
    "                if row > 0:\n",
    "                    H[row - 1, column - 4] = 1\n",
    "        return H\n",
    "#############################################\n",
    "def row_reduce(mat, ncols=None):\n",
    "    assert mat.ndim == 2\n",
    "    ncols = mat.shape[1] if ncols is None else ncols\n",
    "    mat_row_reduced = mat.copy()\n",
    "    p = 0\n",
    "    for j in range(ncols):\n",
    "        idxs = p + np.nonzero(mat_row_reduced[p:,j])[0]\n",
    "        if idxs.size == 0:\n",
    "            continue\n",
    "        mat_row_reduced[[p,idxs[0]],:] = mat_row_reduced[[idxs[0],p],:]\n",
    "        idxs = np.nonzero(mat_row_reduced[:,j])[0].tolist()\n",
    "        idxs.remove(p)\n",
    "        mat_row_reduced[idxs,:] = mat_row_reduced[idxs,:] ^ mat_row_reduced[p,:]\n",
    "        p += 1\n",
    "        if p == mat_row_reduced.shape[0]:\n",
    "            break\n",
    "    return mat_row_reduced, p\n",
    "\n",
    "def get_generator(pc_matrix_):\n",
    "    assert pc_matrix_.ndim == 2\n",
    "    pc_matrix = pc_matrix_.copy().astype(bool).transpose()\n",
    "    pc_matrix_I = np.concatenate((pc_matrix, np.eye(pc_matrix.shape[0], dtype=bool)), axis=-1)\n",
    "    pc_matrix_I, p = row_reduce(pc_matrix_I, ncols=pc_matrix.shape[1])\n",
    "    return row_reduce(pc_matrix_I[p:,pc_matrix.shape[1]:])[0]\n",
    "\n",
    "def get_standard_form(pc_matrix_):\n",
    "    pc_matrix = pc_matrix_.copy().astype(bool)\n",
    "    next_col = min(pc_matrix.shape)\n",
    "    for ii in range(min(pc_matrix.shape)):\n",
    "        while True:\n",
    "            rows_ones = ii + np.where(pc_matrix[ii:, ii])[0]\n",
    "            if len(rows_ones) == 0:\n",
    "                new_shift = np.arange(ii, min(pc_matrix.shape) - 1).tolist()+[min(pc_matrix.shape) - 1,next_col]\n",
    "                old_shift = np.arange(ii + 1, min(pc_matrix.shape)).tolist()+[next_col, ii]\n",
    "                pc_matrix[:, new_shift] = pc_matrix[:, old_shift]\n",
    "                next_col += 1\n",
    "            else:\n",
    "                break\n",
    "        pc_matrix[[ii, rows_ones[0]], :] = pc_matrix[[rows_ones[0], ii], :]\n",
    "        other_rows = pc_matrix[:, ii].copy()\n",
    "        other_rows[ii] = False\n",
    "        pc_matrix[other_rows] = pc_matrix[other_rows] ^ pc_matrix[ii]\n",
    "    return pc_matrix.astype(int)\n",
    "#############################################\n",
    "\n",
    "def sign_to_bin(x):\n",
    "    return 0.5 * (1 - x)\n",
    "\n",
    "def bin_to_sign(x):\n",
    "    return 1 - 2 * x\n",
    "\n",
    "def EbN0_to_std(EbN0, rate):\n",
    "    snr =  EbN0 + 10. * np.log10(2 * rate)\n",
    "    return np.sqrt(1. / (10. ** (snr / 10.)))\n",
    "\n",
    "def BER(x_pred, x_gt):\n",
    "    return torch.mean((x_pred != x_gt).float()).item()\n",
    "\n",
    "def FER(x_pred, x_gt):\n",
    "    return torch.mean(torch.any(x_pred != x_gt, dim=1).float()).item()\n",
    "\n",
    "#############################################\n",
    "def Get_Generator_and_Parity(n, k, standard_form = False):\n",
    "    path_pc_mat = os.path.join('ECCT', 'Codes_DB', f'{code.code_type}_N{str(n)}_K{str(k)}')\n",
    "    if code.code_type in ['POLAR', 'BCH']:\n",
    "        ParityMatrix = np.loadtxt(path_pc_mat+'.txt')\n",
    "    elif code.code_type in ['CCSDS', 'LDPC', 'MACKAY']:\n",
    "        ParityMatrix = Read_pc_matrixrix_alist(path_pc_mat+'.alist')\n",
    "    else:\n",
    "        raise Exception(f'Wrong code {code.code_type}')\n",
    "    if standard_form and code.code_type not in ['CCSDS', 'LDPC', 'MACKAY']:\n",
    "        ParityMatrix = get_standard_form(ParityMatrix).astype(int)\n",
    "        GeneratorMatrix = np.concatenate([np.mod(-ParityMatrix[:, min(ParityMatrix.shape):].transpose(),2),np.eye(k)],1).astype(int)\n",
    "    else:\n",
    "        GeneratorMatrix = get_generator(ParityMatrix)\n",
    "    assert np.all(np.mod((np.matmul(GeneratorMatrix, ParityMatrix.transpose())), 2) == 0) and np.sum(GeneratorMatrix) > 0\n",
    "    return GeneratorMatrix.astype(float), ParityMatrix.astype(float)\n",
    "\n",
    "class Code():\n",
    "        pass\n",
    "code = Code()\n",
    "code.k = 24\n",
    "code.n = 49\n",
    "code.code_type = 'LDPC'\n",
    "G, H = Get_Generator_and_Parity(code.n, code.k)\n",
    "code.generator_matrix = torch.from_numpy(G).transpose(0, 1).long()\n",
    "code.pc_matrix = torch.from_numpy(H).long()\n",
    "# args.code = codecode = Code()\n",
    "code.pc_matrix.size()\n",
    "\n",
    "# Goal, how can we have a **few shot** learner??? I.e. how can we feed the code __into___ the transformer\n",
    "# Maybe we can \"encode\" parity check matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECCT\t     codey_pylib.py\t\tquick_start_pytorch_images\n",
      "README.md    model_out\t\t\ttest.txt\n",
      "__pycache__  quick_start_pytorch.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 4, 2]),\n",
       " tensor([[[1., 1.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [1., 1.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [1., 1.],\n",
       "          [0., 0.]]]),\n",
       " tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.cat(2 * [torch.eye(3, 4).unsqueeze(0).unsqueeze(-1)], dim=-1) * torch.cat(3 * [torch.eye(3, 4).unsqueeze(0)]).unsqueeze(-1)\n",
    "a.size(), a[1], torch.eye(3, 4) * torch.eye(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Modified Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Yoni Choukroun, choukroun.yoni@gmail.com\n",
    "Error Correction Code Transformer\n",
    "https://arxiv.org/abs/2203.14966\n",
    "\"\"\"\n",
    "from torch.nn import LayerNorm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        if N > 1:\n",
    "            self.norm2 = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for idx, layer in enumerate(self.layers, start=1):\n",
    "            x = layer(x, mask)\n",
    "            if idx == len(self.layers)//2 and len(self.layers) > 1:\n",
    "                x = self.norm2(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        nbatches = query.size(0)\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "            / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if self.dropout is not None:\n",
    "            p_attn = self.dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.gelu(self.w_1(x))))\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "class ECC_Transformer(nn.Module):\n",
    "    def __init__(self, n, k, h, d_model, N_dec, pc_adj_size, dropout=0):\n",
    "        super(ECC_Transformer, self).__init__()\n",
    "        ####\n",
    "        c = copy.deepcopy\n",
    "        attn = MultiHeadedAttention(h, d_model)\n",
    "        ff = PositionwiseFeedForward(d_model, d_model*4, dropout)\n",
    "\n",
    "        # MODIFIED\n",
    "        self.src_embed = torch.nn.Parameter(torch.empty(\n",
    "            (pc_adj_size + n + (n - k), d_model)))\n",
    "        # END MODIFIED\n",
    "\n",
    "        self.decoder = Encoder(EncoderLayer(\n",
    "            d_model, c(attn), c(ff), dropout), N_dec)\n",
    "        self.oned_final_embed = torch.nn.Sequential(\n",
    "            *[nn.Linear(d_model, 1)])\n",
    "\n",
    "        self.out_fc = nn.Linear(n + (n - k) + pc_adj_size, 1)\n",
    "\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.pc_adj_size = pc_adj_size\n",
    "        # TODO: renable mask\n",
    "        # logging.info(f'Mask:\\n {self.src_mask}')\n",
    "        ###\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # Only allow 1 batch size for now b/c I am feeling lazy rn\n",
    "    def forward(self, p_check_mat, p_check_mat_adj,  magnitude, syndrome):\n",
    "        # Modified\n",
    "        emb = torch.cat([magnitude,\n",
    "                        syndrome, p_check_mat_adj], -1).unsqueeze(-1)\n",
    "        # emb = torch.cat(emb.size()[0] * [self.src_embed.unsqueeze(-3)]) * emb\n",
    "        emb = self.src_embed.unsqueeze(0) * emb\n",
    "        self.get_mask(self.n, self.k, p_check_mat.view(-1,\n",
    "                      self.n - self.k, self.n).squeeze(0))  # TODO: reintroduce mask\n",
    "        # Hm... I feel like this ain't gonna work\n",
    "        # emb = emb[:, -1 * (2 * self.n - self.k):, :]\n",
    "        emb = self.decoder(emb, self.src_mask)  # TODO: SRC_MASK\n",
    "        return self.out_fc(self.oned_final_embed(emb).squeeze(-1))\n",
    "\n",
    "    def loss(self, z_pred, z2, y):\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            z_pred, sign_to_bin(torch.sign(z2)))\n",
    "        x_pred = sign_to_bin(torch.sign(-z_pred * torch.sign(y)))\n",
    "        return loss, x_pred\n",
    "\n",
    "    def get_mask(self, n, k, p_check_mat, no_mask=False):\n",
    "        p_check_size = n * (n - k)\n",
    "        if no_mask:\n",
    "            # self.src_mask = None\n",
    "            return\n",
    "\n",
    "        mask_size_col = self.pc_adj_size + n + n - k\n",
    "        mask_size_row = self.pc_adj_size + n + n - k\n",
    "\n",
    "        def build_mask():\n",
    "            # Set the mask to \"on\" for all the p_checks\n",
    "            # TODO: WAIT A SECOND: HOW DOES THIS NOW WORK? WE ARE CATTING ON 0 VECTOR HERE.... b/c col-row = 0\n",
    "            mask = torch.cat([torch.eye(mask_size_row, mask_size_row), torch.ones(mask_size_row, mask_size_col -\n",
    "                             mask_size_row)], dim=1)\n",
    "            # mask = torch.eye(mask_size_row, mask_size_row)\n",
    "            for ii in range(n-k):\n",
    "                idx = torch.where(p_check_mat[ii] > 0)[0]\n",
    "                for jj in idx:\n",
    "                    for kk in idx:\n",
    "                        if jj != kk:\n",
    "                            mask[jj,   kk] += 1\n",
    "                            mask[kk,  jj] += 1\n",
    "                            mask[n + ii,  jj] += 1\n",
    "                            mask[jj,  n + ii] += 1\n",
    "            src_mask = ~ (mask > 0).unsqueeze(0).unsqueeze(0)\n",
    "            return src_mask\n",
    "        src_mask = build_mask()\n",
    "        a = mask_size_row * mask_size_col\n",
    "        logging.info(\n",
    "            f'Self-Attention Sparsity Ratio={100 * torch.sum((src_mask).int()) / a:0.2f}%, Self-Attention Complexity Ratio={100 * torch.sum((~src_mask).int())//2 / a:0.2f}%')\n",
    "        self.register_buffer('src_mask', src_mask.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.),\n",
       " tensor([ 0, 74, 82, 86, 90,  1, 73, 79, 93, 98,  2, 80, 83, 92, 97,  3, 71, 72,\n",
       "         79, 94,  4, 78, 81, 92, 97,  5, 79, 89, 90, 97,  6, 70, 72, 91, 95,  7,\n",
       "         82, 87, 88, 96,  8, 72, 76, 77, 79,  9, 71, 75, 88, 98, 10, 72, 82, 91,\n",
       "         97, 11, 73, 88, 92, 99, 12, 70, 81, 94, 99, 13, 72, 79, 90, 96, 14, 71,\n",
       "         89, 92, 98, 15, 81, 88, 94, 99, 16, 77, 81, 97, 99, 17, 84, 91, 92, 96,\n",
       "         18, 72, 85, 88, 93, 19, 77, 80, 88, 91, 20, 72, 75, 91, 94, 21, 77, 78,\n",
       "         86, 88, 22, 82, 86, 91, 98, 23, 82, 85, 91, 99, 24, 77, 90, 91, 95, 25,\n",
       "         76, 82, 94, 95, 26, 70, 73, 75, 82, 27, 71, 73, 74, 75, 28, 77, 90, 91,\n",
       "         95, 29, 72, 77, 92, 96, 30, 70, 71, 84, 93, 31, 75, 76, 89, 94, 32, 88,\n",
       "         90, 95, 99, 33, 75, 77, 91, 95, 34, 72, 76, 90, 94, 35, 83, 86, 91, 92,\n",
       "         36, 77, 81, 98, 99, 37, 70, 74, 76, 97, 38, 79, 84, 90, 98, 39, 73, 84,\n",
       "         89, 91, 40, 77, 82, 85, 99, 41, 71, 72, 87, 93, 42, 84, 93, 95, 97, 43,\n",
       "         79, 86, 88, 99, 44, 75, 80, 96, 97, 45, 83, 85, 89, 93, 46, 71, 80, 86,\n",
       "         89, 47, 70, 85, 90, 98, 48, 78, 79, 85, 93, 49, 71, 73, 94, 99, 50, 70,\n",
       "         83, 89, 99, 51, 73, 82, 96, 99, 52, 76, 79, 95, 99, 53, 72, 77, 81, 94,\n",
       "         54, 72, 84, 89, 95, 55, 72, 74, 78, 95, 56, 72, 87, 92, 99, 57, 72, 75,\n",
       "         80, 82, 58, 71, 84, 88, 95, 59, 82, 86, 89, 99, 60, 70, 79, 81, 83, 61,\n",
       "         71, 94, 95, 97, 62, 79, 81, 89, 94, 63, 82, 85, 92, 94, 64, 92, 93, 96,\n",
       "         99, 65, 76, 88, 93, 98, 66, 71, 82, 84, 88, 67, 80, 90, 96, 97, 68, 77,\n",
       "         95, 97, 99, 69, 80, 84, 87, 91]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thanks to https://github.com/christiansiegel/coding-theory-algorithms/blob/master/LinearBlockCode.py\n",
    "def GtoP(G):\n",
    "    \"\"\"Extract the submatrix P from a Generator Matrix in systematic form.\n",
    "    Args:\n",
    "        G: Generator Matrix in systematic form\n",
    "    Returns:\n",
    "        Submatrix P of G.\n",
    "    \"\"\"\n",
    "    k = G.size(0)\n",
    "    n = G.size(1)\n",
    "    P = G[:k, :n - k]\n",
    "    return P\n",
    "\n",
    "\n",
    "def HtoP(H):\n",
    "    \"\"\"Extract the submatrix P from a Parity Check Matrix in systematic form.\n",
    "    Args:\n",
    "        H: Parity Check Matrix in systematic form\n",
    "    Returns:\n",
    "        Submatrix P of G.\n",
    "    \"\"\"\n",
    "    n = H.size(1)\n",
    "    k = n - H.size(0)\n",
    "    PK = H[:, n - k:n]\n",
    "    P = PK.transpose(0, 1)\n",
    "    return P\n",
    "\n",
    "\n",
    "def HtoG(H):\n",
    "    \"\"\"Convert a Parity Check Matrix in systematic form to a Generator Matrix.\n",
    "    Args:\n",
    "        H: Parity Check Matrix in systematic form\n",
    "    Returns:\n",
    "        Generator Matrix G\n",
    "    \"\"\"\n",
    "    n = H.size(1)\n",
    "    k = n - H.size(0)\n",
    "    P = HtoP(H)\n",
    "    Ik = torch.eye(k)\n",
    "    G = torch.concat((P, Ik), axis=1)\n",
    "    return G\n",
    "\n",
    "\n",
    "n = 100\n",
    "k = 30\n",
    "dR = 5\n",
    "\n",
    "\n",
    "def gen_random_ldpc(n, k, deg_row):\n",
    "    iden_left = torch.eye(n - k)\n",
    "    x = torch.zeros(n-k, k)\n",
    "    x[:, :(deg_row - 1)] = 1\n",
    "    perm = torch.stack([torch.randperm(k) for _ in range((n - k))])\n",
    "    x = x.gather(dim=1, index=perm)\n",
    "    # TODO: above is wrong use first approach\n",
    "    rand_ldpc_H = torch.concat([iden_left, x], dim=1)\n",
    "    G = HtoG(rand_ldpc_H)\n",
    "    return rand_ldpc_H, G\n",
    "\n",
    "def pc_to_adj(H):\n",
    "    x = H.nonzero()\n",
    "    return x[:, 1]\n",
    "\n",
    "rand_ldpc_H, G = gen_random_ldpc(n, k, dR)\n",
    "g = (torch.bernoulli(torch.Tensor([0.1]).repeat(1, k)) @ G) % 2\n",
    "# Yay, it works :)\n",
    "((rand_ldpc_H @ G.transpose(0, 1)) % 2).sum(), pc_to_adj(rand_ldpc_H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This important will remove all the above stuff..\n",
    "if False:\n",
    "\tfrom codey_pylib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7000]), torch.Size([100]), torch.Size([70]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of \"Error Correction Code Transformer\" (ECCT)\n",
    "https://arxiv.org/abs/2203.14966\n",
    "@author: Yoni Choukroun, choukroun.yoni@gmail.com\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class ECC_Dataset(data.Dataset):\n",
    "    def __init__(self, n, k, sigma, len, zero_cw=True, deg_row=5, always_shuffle_code=False):\n",
    "        self.sigma = sigma\n",
    "        self.len = len\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        # self.generator_matrix = code.generator_matrix.transpose(0, 1)\n",
    "        # self.pc_matrix = code.pc_matrix.transpose(0, 1)\n",
    "        self.deg_row = deg_row\n",
    "        self.zero_word = torch.zeros((self.k)).long() if zero_cw else None\n",
    "        self.zero_cw = torch.zeros((self.n)).long() if zero_cw else None\n",
    "        self.always_shuffle_code = always_shuffle_code\n",
    "        self.shuffle_rand_code()\n",
    "    \n",
    "    def shuffle_rand_code(self):\n",
    "        pc_matrix, generator_matrix = gen_random_ldpc(self.n, self.k, self.deg_row)\n",
    "        self.pc_matrix = pc_matrix\n",
    "        self.generator_matrix = generator_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.always_shuffle_code:\n",
    "            self.shuffle_rand_code()\n",
    "        if self.zero_cw is None:\n",
    "            m = torch.randint(0, 2, (1, self.k)).squeeze().unsqueeze(-2)\n",
    "            x = torch.matmul(m.float(), self.generator_matrix).transpose(-1, -2) % 2\n",
    "            x = x.squeeze(-1)\n",
    "        else:\n",
    "            m = self.zero_word\n",
    "            x = self.zero_cw\n",
    "        z = torch.randn(self.n) * random.choice(self.sigma)\n",
    "        y = bin_to_sign(x) + z\n",
    "        magnitude = torch.abs(y)\n",
    "        syndrome = torch.matmul(self.pc_matrix.long(), sign_to_bin(torch.sign(y)).long().unsqueeze(-1)) % 2\n",
    "        syndrome = bin_to_sign(syndrome).squeeze(-1)\n",
    "        return self.pc_matrix.flatten().float(), pc_to_adj(self.pc_matrix), m.float(), x.float(), z.float(), y.float(), magnitude.float(), syndrome.float()\n",
    "\n",
    "dataset = ECC_Dataset(n, k, [1], 1)\n",
    "a = dataset.__getitem__(1)\n",
    "a[0].size(),  a[-2].size(), a[-1].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/1223232003.py:182: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  f'Self-Attention Sparsity Ratio={100 * torch.sum((src_mask).int()) / a:0.2f}%, Self-Attention Complexity Ratio={100 * torch.sum((~src_mask).int())//2 / a:0.2f}%')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 1., 1., 0., 0., 0., 1., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 1\n",
    "N_dec = 1 # CHanged from 6\n",
    "d_model = 20 # default is 32 but we are adding parity check info...\n",
    "pc_adj_size = pc_to_adj( gen_random_ldpc(n, k, deg_row=5)[0]).size()[-1]\n",
    "test_model = ECC_Transformer(n, k, h, d_model, N_dec, pc_adj_size, dropout=0).to(device)\n",
    "r = None\n",
    "with torch.no_grad():\n",
    "\tr = test_model(a[0].to(device), a[1].to(device), a[6].to(device), a[7].to(device))\n",
    "r.heaviside(torch.Tensor([0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1], [3, 2]])[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/1223232003.py:182: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  f'Self-Attention Sparsity Ratio={100 * torch.sum((src_mask).int()) / a:0.2f}%, Self-Attention Complexity Ratio={100 * torch.sum((~src_mask).int())//2 / a:0.2f}%')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FER count threshold reached for EbN0:4\n",
      "Test EbN0=4, BER=1.13e-01\n",
      "FER count threshold reached for EbN0:5\n",
      "Test EbN0=5, BER=9.50e-02\n",
      "FER count threshold reached for EbN0:6\n",
      "Test EbN0=6, BER=4.46e-02\n",
      "Losses for test of [0.3594899096272208, 0.30780491046607494, 0.19489356474234507] [0.11272727020762184, 0.09499999756614368, 0.04461538333159227] [1.0, 0.9166666666666666, 0.8461538461538461]\n",
      "Epoch 1 finished, loss: 0.3049606670066714, ber: 0.09449999757111073, fer: 0.97\n",
      "Epoch 2 finished, loss: 0.3273327129147947, ber: 0.10349999709054827, fer: 0.965\n",
      "Epoch 3 finished, loss: 0.2972557982429862, ber: 0.0924999972525984, fer: 0.9575\n",
      "Epoch 4 finished, loss: 0.3024458884168416, ber: 0.09479999737814068, fer: 0.96\n",
      "Epoch 5 finished, loss: 0.3161900403071195, ber: 0.10189999711699783, fer: 0.97\n",
      "Epoch 6 finished, loss: 0.31235774494707585, ber: 0.10054999725893139, fer: 0.9825\n",
      "Epoch 7 finished, loss: 0.3207790886517614, ber: 0.10514999704435468, fer: 0.97\n",
      "Epoch 8 finished, loss: 0.31178453031461684, ber: 0.10214999720454215, fer: 0.965\n",
      "Epoch 9 finished, loss: 0.30432195307221266, ber: 0.09889999727718532, fer: 0.98\n",
      "Epoch 10 finished, loss: 0.30398563263006506, ber: 0.0988999973796308, fer: 0.97\n",
      "Epoch 11 finished, loss: 0.30625822791364044, ber: 0.10009999713860453, fer: 0.9625\n",
      "Epoch 12 finished, loss: 0.2965930040180683, ber: 0.09739999745972455, fer: 0.9775\n",
      "Epoch 13 finished, loss: 0.3045136270625517, ber: 0.10124999706633389, fer: 0.9625\n",
      "Epoch 14 finished, loss: 0.2974230669019744, ber: 0.09849999720230698, fer: 0.97\n",
      "Saving Model at Eopch 15\n",
      "Epoch 15 finished, loss: 0.2960920597799122, ber: 0.09874999747611582, fer: 0.9625\n",
      "Epoch 16 finished, loss: 0.29936528839170934, ber: 0.1015499973949045, fer: 0.98\n",
      "Epoch 17 finished, loss: 0.29854078890988606, ber: 0.10279999700374902, fer: 0.9725\n",
      "Epoch 18 finished, loss: 0.3047934380546212, ber: 0.10504999733529985, fer: 0.97\n",
      "Epoch 19 finished, loss: 0.27140338928671554, ber: 0.09394999741576612, fer: 0.9725\n",
      "Epoch 20 finished, loss: 0.26893961447756737, ber: 0.09289999723434449, fer: 0.94\n",
      "Epoch 21 finished, loss: 0.2673312410339713, ber: 0.09189999740570784, fer: 0.925\n",
      "Epoch 22 finished, loss: 0.26748502247268335, ber: 0.0935999972652644, fer: 0.9475\n",
      "Epoch 23 finished, loss: 0.246535063192714, ber: 0.08644999769516289, fer: 0.9075\n",
      "Epoch 24 finished, loss: 0.2733150979934726, ber: 0.09619999735616147, fer: 0.9\n",
      "Epoch 25 finished, loss: 0.25344787805108354, ber: 0.09084999741055072, fer: 0.9175\n",
      "Epoch 26 finished, loss: 0.25003388545941563, ber: 0.08869999779388309, fer: 0.8925\n",
      "Epoch 27 finished, loss: 0.24644005524460227, ber: 0.08879999746568501, fer: 0.8825\n",
      "Epoch 28 finished, loss: 0.2528557020495646, ber: 0.09064999734982848, fer: 0.8775\n",
      "Epoch 29 finished, loss: 0.24218913825345226, ber: 0.08469999755732716, fer: 0.875\n",
      "Epoch 30 finished, loss: 0.25093346141336953, ber: 0.09154999751597642, fer: 0.8475\n",
      "Saving Model at Eopch 31\n",
      "Epoch 31 finished, loss: 0.2272395160223823, ber: 0.08259999764151871, fer: 0.8475\n",
      "Epoch 32 finished, loss: 0.2348015202296665, ber: 0.08464999782852828, fer: 0.8425\n",
      "Epoch 33 finished, loss: 0.21720656637393404, ber: 0.08034999773837626, fer: 0.815\n",
      "Epoch 34 finished, loss: 0.20929193431278692, ber: 0.07614999788813293, fer: 0.805\n",
      "Epoch 35 finished, loss: 0.21445125363097758, ber: 0.07824999769218266, fer: 0.79\n",
      "Epoch 36 finished, loss: 0.21378451779368335, ber: 0.07879999792203307, fer: 0.795\n",
      "Epoch 37 finished, loss: 0.2229704263745225, ber: 0.08299999780952931, fer: 0.79\n",
      "Epoch 38 finished, loss: 0.21354323736741207, ber: 0.07854999780654907, fer: 0.775\n",
      "Epoch 39 finished, loss: 0.2112898042955203, ber: 0.07799999791197479, fer: 0.75\n",
      "Epoch 40 finished, loss: 0.21186398128164, ber: 0.07939999782480299, fer: 0.755\n",
      "Epoch 41 finished, loss: 0.2150928791964543, ber: 0.07994999777525663, fer: 0.76\n",
      "Epoch 42 finished, loss: 0.19092651999671945, ber: 0.06984999813139439, fer: 0.6975\n",
      "Epoch 43 finished, loss: 0.19382914025045467, ber: 0.0707999980263412, fer: 0.7125\n",
      "Epoch 44 finished, loss: 0.18872018624359044, ber: 0.07009999788366258, fer: 0.685\n",
      "Epoch 45 finished, loss: 0.1884862749144668, ber: 0.06979999821633101, fer: 0.6775\n",
      "Epoch 46 finished, loss: 0.1933691593777621, ber: 0.07134999791160226, fer: 0.695\n",
      "Saving Model at Eopch 47\n",
      "Epoch 47 finished, loss: 0.17508605108683695, ber: 0.06384999802336097, fer: 0.6575\n",
      "Epoch 48 finished, loss: 0.19776063426965265, ber: 0.07374999813735485, fer: 0.6625\n",
      "Epoch 49 finished, loss: 0.1744022390009195, ber: 0.06299999831244349, fer: 0.615\n",
      "Epoch 50 finished, loss: 0.18938382929351064, ber: 0.06819999798201025, fer: 0.665\n",
      "Epoch 51 finished, loss: 0.16251810409798054, ber: 0.05954999833367765, fer: 0.6175\n",
      "Epoch 52 finished, loss: 0.14790116210177076, ber: 0.0541999985743314, fer: 0.585\n",
      "Epoch 53 finished, loss: 0.16188001061760587, ber: 0.05829999843612313, fer: 0.605\n",
      "Epoch 54 finished, loss: 0.17080650937670724, ber: 0.06409999823197722, fer: 0.64\n",
      "Epoch 55 finished, loss: 0.16227990906816558, ber: 0.0598499983549118, fer: 0.535\n",
      "Epoch 56 finished, loss: 0.17340500636331854, ber: 0.06389999839477241, fer: 0.56\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "##################################################################\n",
    "##################################################################\n",
    "# logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)\n",
    "# logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, LR, shuffle_code_round):\n",
    "    model.train()\n",
    "    cum_loss = cum_ber = cum_fer = cum_samples = 0\n",
    "    t = time.time()\n",
    "    for batch_idx, (parity_check, pc_adj, m, x, z, y, magnitude, syndrome) in enumerate(\n",
    "            train_loader):\n",
    "        if batch_idx % shuffle_code_round == 0:\n",
    "            train_loader.dataset.shuffle_rand_code()\n",
    "        z_mul = (y * bin_to_sign(x))\n",
    "        z_pred = model(parity_check.to(device), pc_adj.to(device),\n",
    "                       magnitude.to(device), syndrome.to(device))\n",
    "        loss, x_pred = model.loss(-z_pred, z_mul.to(device), y.to(device))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ###\n",
    "        ber = BER(x_pred, x.to(device))\n",
    "        fer = FER(x_pred, x.to(device))\n",
    "\n",
    "        cum_loss += loss.item() * x.shape[0]\n",
    "        cum_ber += ber * x.shape[0]\n",
    "        cum_fer += fer * x.shape[0]\n",
    "        cum_samples += x.shape[0]\n",
    "        if (batch_idx+1) % 500 == 0 or batch_idx == len(train_loader) - 1:\n",
    "            logging.info(\n",
    "                f'Training epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}: LR={LR:.2e}, Loss={cum_loss / cum_samples:.2e} BER={cum_ber / cum_samples:.2e} FER={cum_fer / cum_samples:.2e}')\n",
    "    logging.info(f'Epoch {epoch} Train Time {time.time() - t}s\\n')\n",
    "    return cum_loss / cum_samples, cum_ber / cum_samples, cum_fer / cum_samples\n",
    "\n",
    "\n",
    "##################################################################\n",
    "\n",
    "def test(model, device, test_loader_list, EbNo_range_test, min_FER=10):\n",
    "    model.eval()\n",
    "    test_loss_list, test_loss_ber_list, test_loss_fer_list, cum_samples_all = [], [], [], []\n",
    "    t = time.time()\n",
    "    with torch.no_grad():\n",
    "        for ii, test_loader in enumerate(test_loader_list):\n",
    "            test_loss = test_ber = test_fer = cum_count = 0.\n",
    "            while True:\n",
    "                (parity_check, pc_adj, jm, x, z, y, magnitude,\n",
    "                 syndrome) = next(iter(test_loader))\n",
    "                z_mul = (y * bin_to_sign(x))\n",
    "                z_pred = model(parity_check.to(device), pc_adj.to(device),\n",
    "                               magnitude.to(device), syndrome.to(device))\n",
    "                loss, x_pred = model.loss(-z_pred,\n",
    "                                          z_mul.to(device), y.to(device))\n",
    "\n",
    "                test_loss += loss.item() * x.shape[0]\n",
    "\n",
    "                test_ber += BER(x_pred, x.to(device)) * x.shape[0]\n",
    "                test_fer += FER(x_pred, x.to(device)) * x.shape[0]\n",
    "                cum_count += x.shape[0]\n",
    "                # cum count before 1e5\n",
    "                if (min_FER > 0 and test_fer > min_FER and cum_count > 10) or cum_count >= 1e9:\n",
    "                    if cum_count >= 1e9:\n",
    "                        print(\n",
    "                            f'Number of samples threshold reached for EbN0:{EbNo_range_test[ii]}')\n",
    "                    else:\n",
    "                        print(\n",
    "                            f'FER count threshold reached for EbN0:{EbNo_range_test[ii]}')\n",
    "                    break\n",
    "            cum_samples_all.append(cum_count)\n",
    "            test_loss_list.append(test_loss / cum_count)\n",
    "            test_loss_ber_list.append(test_ber / cum_count)\n",
    "            test_loss_fer_list.append(test_fer / cum_count)\n",
    "            print(\n",
    "                f'Test EbN0={EbNo_range_test[ii]}, BER={test_loss_ber_list[-1]:.2e}')\n",
    "        ###\n",
    "        logging.info('\\nTest Loss ' + ' '.join(\n",
    "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
    "             in\n",
    "             (zip(test_loss_list, EbNo_range_test))]))\n",
    "        logging.info('Test FER ' + ' '.join(\n",
    "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
    "             in\n",
    "             (zip(test_loss_fer_list, EbNo_range_test))]))\n",
    "        logging.info('Test BER ' + ' '.join(\n",
    "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
    "             in\n",
    "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
    "        logging.info('Test -ln(BER) ' + ' '.join(\n",
    "            ['{}: {:.2e}'.format(ebno, -np.log(elem)) for (elem, ebno)\n",
    "             in\n",
    "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
    "    logging.info(\n",
    "        f'# of testing samples: {cum_samples_all}\\n Test Time {time.time() - t} s\\n')\n",
    "    return test_loss_list, test_loss_ber_list, test_loss_fer_list\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "model = None\n",
    "\n",
    "\n",
    "def main(n, k, deg_row, save_path,):\n",
    "    global model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #################################\n",
    "    h = 4  # changed from 8...\n",
    "    lr = 1e-4\n",
    "    epochs = 1000\n",
    "    batch_size = 1\n",
    "    # Use a new random code after 32 runs, we do not want this to be too high as we are\n",
    "    # trying to learn a **general** decoder\n",
    "    rand_code_shuffle_len = 8\n",
    "    workers = 1\n",
    "    N_dec = 3  # CHanged from 6\n",
    "    d_model = 40  # default is 32 but we are adding parity check info...\n",
    "    adj_size = (n - k) * deg_row\n",
    "    model = ECC_Transformer(n, k, h, d_model, N_dec,\n",
    "                            adj_size,  dropout=0).to(device)\n",
    "    model = torch.load(os.path.join(save_path, 'best_model'))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # We want a train size of about 400 * batch_size\n",
    "    train_size = batch_size * \\\n",
    "        math.floor(400 / rand_code_shuffle_len) * rand_code_shuffle_len\n",
    "\n",
    "    logging.info(model)\n",
    "    logging.info(\n",
    "        f'# of Parameters: {np.sum([np.prod(p.shape) for p in model.parameters()])}')\n",
    "    #################################\n",
    "    EbNo_range_test = range(4, 7)\n",
    "    EbNo_range_train = range(2, 8)\n",
    "    test_batch_size = 1\n",
    "    std_train = [EbN0_to_std(ii, k / n) for ii in EbNo_range_train]\n",
    "    std_test = [EbN0_to_std(ii, k / n) for ii in EbNo_range_test]\n",
    "    train_dataloader = DataLoader(ECC_Dataset(n, k, std_train, len=train_size, deg_row=deg_row, zero_cw=True, always_shuffle_code=False), batch_size=int(batch_size),\n",
    "                                  shuffle=True, num_workers=workers)\n",
    "    test_dataloader_list = [DataLoader(ECC_Dataset(n, k, [std_test[ii]], deg_row=deg_row, len=int(test_batch_size), zero_cw=False, always_shuffle_code=True),\n",
    "                                       batch_size=int(test_batch_size), shuffle=False, num_workers=workers) for ii in range(len(std_test))]\n",
    "    #################################\n",
    "    # TODO: increase the batch size so loss is a better metric for saving\n",
    "    best_loss = float('inf')\n",
    "    last_save_epoch = -1\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss, ber, fer = train(model, device, train_dataloader, optimizer,\n",
    "                               epoch, LR=scheduler.get_last_lr()[0], shuffle_code_round=rand_code_shuffle_len)\n",
    "        scheduler.step()\n",
    "        if loss < best_loss and epoch - last_save_epoch > 15:\n",
    "            best_loss = loss\n",
    "            last_save_epoch = epoch\n",
    "            torch.save(model, os.path.join(save_path, 'best_model'))\n",
    "            print(\"Saving Model at Eopch\", epoch)\n",
    "        if epoch % 300 == 0 or epoch in [1, epochs]:\n",
    "            test_loss_list, test_loss_ber_list, test_loss_fer_list = test(\n",
    "                model, device, test_dataloader_list, EbNo_range_test)\n",
    "            print(\"Losses for test of\", test_loss_list,\n",
    "                  test_loss_ber_list, test_loss_fer_list)\n",
    "        # clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch} finished, loss: {loss}, ber: {ber}, fer: {fer}\")\n",
    "\n",
    "\n",
    "GLOBAL_N = 50\n",
    "GLOBAL_K = 15\n",
    "GLOBAL_DEG_ROW = 5\n",
    "main(GLOBAL_N, GLOBAL_K, GLOBAL_DEG_ROW, 'model_out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\ttorch.save(model, os.path.join('model_out', 'best_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ECCT/Main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning test\n",
    "\n",
    "Fine tune the transformer to work on only **1** specific ECC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ECC_DatasetFineTune(data.Dataset):\n",
    "    def __init__(self, n, k, sigma, len, parity_check_mat, gen_mat, zero_cw=True):\n",
    "        self.sigma = sigma\n",
    "        self.len = len\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        # self.generator_matrix = code.generator_matrix.transpose(0, 1)\n",
    "        # self.pc_matrix = code.pc_matrix.transpose(0, 1)\n",
    "        self.zero_word = torch.zeros((self.k)).long() if zero_cw else None\n",
    "        self.zero_cw = torch.zeros((self.n)).long() if zero_cw else None\n",
    "        self.pc_mat = parity_check_mat\n",
    "        self.gen_mat = gen_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        deg_row_avg = 5  # TODO: vary me\n",
    "        pc_matrix, generator_matrix = self.pc_mat, self.gen_mat\n",
    "        if self.zero_cw is None:\n",
    "            m = torch.randint(0, 2, (1, self.k)).squeeze().unsqueeze(-2)\n",
    "            x = torch.matmul(m.float(), generator_matrix).transpose(-1, -2) % 2\n",
    "            x = x.squeeze(-1)\n",
    "        else:\n",
    "            m = self.zero_word\n",
    "            x = self.zero_cw\n",
    "        z = torch.randn(self.n) * random.choice(self.sigma)\n",
    "        y = bin_to_sign(x) + z\n",
    "        magnitude = torch.abs(y)\n",
    "        syndrome = torch.matmul(pc_matrix.long(), sign_to_bin(\n",
    "            torch.sign(y)).long().unsqueeze(-1)) % 2\n",
    "        syndrome = bin_to_sign(syndrome).squeeze(-1)\n",
    "        return pc_matrix.flatten().float(), m.float(), x.float(), z.float(), y.float(), magnitude.float(), syndrome.float()\n",
    "\n",
    "\n",
    "n = 40\n",
    "k = 5\n",
    "deg_row_avg = 5\n",
    "pc_matrix, generator_matrix = gen_random_ldpc(n, k, deg_row_avg)\n",
    "dataset = ECC_DatasetFineTune(n, k, [1], 1, pc_matrix, generator_matrix)\n",
    "a = dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#################################\n",
    "h = 4  # changed from 8...\n",
    "lr = 1e-4\n",
    "epochs = 1000\n",
    "batch_size = 1\n",
    "workers = 1\n",
    "N_dec = 3  # CHanged from 6\n",
    "d_model = 40  # default is 32 but we are adding parity check info...\n",
    "model = ECC_Transformer(n, k, h, d_model, N_dec, dropout=0).to(device)\n",
    "save_path = 'model_out'\n",
    "model = torch.load(os.path.join(save_path, 'best_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "logging.info(model)\n",
    "logging.info(\n",
    "    f'# of Parameters: {np.sum([np.prod(p.shape) for p in model.parameters()])}')\n",
    "#################################\n",
    "EbNo_range_test = range(4, 7)\n",
    "EbNo_range_train = range(2, 8)\n",
    "test_batch_size = 1\n",
    "std_train = [EbN0_to_std(ii, k / n) for ii in EbNo_range_train]\n",
    "std_test = [EbN0_to_std(ii, k / n) for ii in EbNo_range_test]\n",
    "train_dataloader = DataLoader(ECC_DatasetFineTune(n, k, std_train, batch_size * 20, pc_matrix, generator_matrix, zero_cw=True), batch_size=int(batch_size),\n",
    "                              shuffle=True, num_workers=workers)\n",
    "test_dataloader_list = [DataLoader(ECC_DatasetFineTune(n, k, [std_test[ii]], int(test_batch_size), pc_matrix, generator_matrix,  zero_cw=False),\n",
    "                                   batch_size=int(test_batch_size), shuffle=False, num_workers=workers) for ii in range(len(std_test))]\n",
    "#################################\n",
    "best_loss = float('inf')\n",
    "last_save_epoch = -1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss, ber, fer = train(model, device, train_dataloader, optimizer,\n",
    "                           epoch, LR=scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "    if loss < best_loss and epoch - last_save_epoch > 30:\n",
    "        best_loss = loss\n",
    "        last_save_epoch = epoch\n",
    "        torch.save(model, os.path.join(save_path, 'best_model_fine_tuned'))\n",
    "    if epoch % 300 == 0 or epoch in [1, epochs]:\n",
    "        test_loss_list, test_loss_ber_list, test_loss_fer_list = test(\n",
    "            model, device, test_dataloader_list, EbNo_range_test)\n",
    "        print(\"Losses for test of\", test_loss_list,\n",
    "              test_loss_ber_list, test_loss_fer_list)\n",
    "    # clear_output(wait=True)\n",
    "    print(f\"Epoch {epoch} finished\", loss, ber, fer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
