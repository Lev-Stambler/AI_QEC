{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stable_baselines3[extra] -q\n",
    "# !pip install pyglet==1.5.27 -q\n",
    "# !pip install -U bposd -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the common library for CPC codes\n",
    "import os\n",
    "import sys\n",
    "# TODO: lets do something better here like refactor the common parts and different learning mech parts\n",
    "# !export PATH=$PATH:~/.local/bin\n",
    "sys.path.append(os.getcwd() + \"/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFF3CT_PATH = \"/home/lev/.local/share/aff3ct/build/bin\"\n",
    "# !export PATH=/home/lev/.local/share/aff3ct/build/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.pathsep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'CHROME_DESKTOP': 'code-url-handler.desktop',\n",
       "        'CLUTTER_IM_MODULE': 'ibus',\n",
       "        'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus',\n",
       "        'DEFAULTS_PATH': '/usr/share/gconf/i3.default.path',\n",
       "        'DESKTOP_SESSION': 'i3',\n",
       "        'DISPLAY': ':1',\n",
       "        'ELECTRON_NO_ATTACH_CONSOLE': '1',\n",
       "        'GDK_BACKEND': 'x11',\n",
       "        'GDMSESSION': 'i3',\n",
       "        'GPG_AGENT_INFO': '/run/user/1000/gnupg/S.gpg-agent:0:1',\n",
       "        'GTK_IM_MODULE': 'ibus',\n",
       "        'GTK_MODULES': 'gail:atk-bridge',\n",
       "        'HOME': '/home/lev',\n",
       "        'I3SOCK': '/run/user/1000/i3/ipc-socket.5593',\n",
       "        'LANG': 'en_US.UTF-8',\n",
       "        'LOGNAME': 'lev',\n",
       "        'MANDATORY_PATH': '/usr/share/gconf/i3.mandatory.path',\n",
       "        'MATHEMATICA_HOME': '/usr/local/Wolfram/Mathematica/13.0',\n",
       "        'NIX_PROFILES': '/nix/var/nix/profiles/default /home/lev/.nix-profile',\n",
       "        'NIX_SSL_CERT_FILE': '/etc/ssl/certs/ca-certificates.crt',\n",
       "        'ORIGINAL_XDG_CURRENT_DESKTOP': 'i3',\n",
       "        'PATH': '/bin:/home/lev/.local/bin:/home/lev/.elan/bin:/home/lev/.nix-profile/bin:/home/lev/.local/share/solana/install/active_release/bin:/home/lev/.cargo/bin:/home/lev/.local/bin:/home/lev/bin:/opt/texbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/home/lev/.elan/bin:/home/lev/.nix-profile/bin:/home/lev/.local/share/solana/install/active_release/bin:/home/lev/.cargo/bin:/home/lev/.local/bin:/home/lev/bin:/opt/texbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/home/lev/.local/share/aff3ct/build/bin:/home/lev/.local/share/aff3ct/build/bin',\n",
       "        'PWD': '/home/lev',\n",
       "        'QT_ACCESSIBILITY': '1',\n",
       "        'QT_IM_MODULE': 'ibus',\n",
       "        'SHELL': '/bin/bash',\n",
       "        'SHLVL': '1',\n",
       "        'SYSTEMD_EXEC_PID': '5163',\n",
       "        'USER': 'lev',\n",
       "        'USERNAME': 'lev',\n",
       "        'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       "        'VSCODE_CLI': '1',\n",
       "        'VSCODE_CODE_CACHE_PATH': '/home/lev/.config/Code/CachedData/92da9481c0904c6adfe372c12da3b7748d74bdcb',\n",
       "        'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       "        'VSCODE_CRASH_REPORTER_SANDBOXED_HINT': '1',\n",
       "        'VSCODE_CWD': '/home/lev',\n",
       "        'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       "        'VSCODE_IPC_HOOK': '/run/user/1000/vscode-ecd073da-1.76-main.sock',\n",
       "        'VSCODE_NLS_CONFIG': '{\"locale\":\"en-us\",\"availableLanguages\":{},\"_languagePackSupport\":true}',\n",
       "        'VSCODE_PID': '8373',\n",
       "        'WINDOWPATH': '2',\n",
       "        'XAUTHORITY': '/run/user/1000/gdm/Xauthority',\n",
       "        'XDG_CONFIG_DIRS': '/etc/xdg/xdg-i3:/etc/xdg',\n",
       "        'XDG_CURRENT_DESKTOP': 'i3',\n",
       "        'XDG_DATA_DIRS': '/usr/share/i3:/usr/share/gnome:/home/lev/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop',\n",
       "        'XDG_RUNTIME_DIR': '/run/user/1000',\n",
       "        'XDG_SEAT': 'seat0',\n",
       "        'XDG_SESSION_CLASS': 'user',\n",
       "        'XDG_SESSION_DESKTOP': 'i3',\n",
       "        'XDG_SESSION_ID': '2',\n",
       "        'XDG_SESSION_TYPE': 'x11',\n",
       "        'XDG_VTNR': '2',\n",
       "        'XMODIFIERS': '@im=ibus',\n",
       "        '_': '/usr/bin/code',\n",
       "        'ELECTRON_RUN_AS_NODE': '1',\n",
       "        'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       "        'PYTHONUNBUFFERED': '1',\n",
       "        'PYTHONIOENCODING': 'utf-8',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       "        'QT_QPA_PLATFORM_PLUGIN_PATH': '/home/lev/.local/lib/python3.10/site-packages/cv2/qt/plugins',\n",
       "        'QT_QPA_FONTDIR': '/home/lev/.local/lib/python3.10/site-packages/cv2/qt/fonts',\n",
       "        'LD_LIBRARY_PATH': '/home/lev/.local/lib/python3.10/site-packages/cv2/../../lib64:',\n",
       "        'TPU_ML_PLATFORM': 'Tensorflow',\n",
       "        'TF2_BEHAVIOR': '1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PATH'] += os.pathsep+AFF3CT_PATH\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aff3ct (Linux 64-bit, g++-11.3, SSE2) v3.0.2\n",
      "Compilation options:\n",
      "  - Precision:         8/16/32/64-bit\n",
      "  - Polar bit packing: on\n",
      "  - Polar bounds:      off\n",
      "  - Terminal colors:   on\n",
      "  - Backtrace:         on\n",
      "  - External strings:  on\n",
      "  - MPI:               off\n",
      "  - GSL:               off\n",
      "  - MKL:               off\n",
      "Copyright (c) 2016-2022 - MIT license.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "aff3ct (Linux 64-bit, g++-11.3, SSE2) v3.0.2\n",
      "Compilation options:\n",
      "  - Precision:         8/16/32/64-bit\n",
      "  - Polar bit packing: on\n",
      "  - Polar bounds:      off\n",
      "  - Terminal colors:   on\n",
      "  - Backtrace:         on\n",
      "  - External strings:  on\n",
      "  - MPI:               off\n",
      "  - GSL:               off\n",
      "  - MKL:               off\n",
      "Copyright (c) 2016-2022 - MIT license.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n"
     ]
    }
   ],
   "source": [
    "!aff3ct -v\n",
    "!{AFF3CT_PATH}/aff3ct -v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RL Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from global_params import params\n",
    "from scoring import score_dataset\n",
    "from CPC import cpc_code, generate_random as gen_random_cpc\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Some quick thoughts:\n",
    "-- Should we start with a specific code each time or always a new random code?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SwapLDPCEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "\n",
    "    def __init__(self, target_succ_rate=0.99):\n",
    "        super(SwapLDPCEnv, self).__init__()\n",
    "\n",
    "        self.target_succ_rate = target_succ_rate\n",
    "        _, m_b, m_p, m_c = gen_random_cpc.random_cpc()\n",
    "        self.m_b = m_b\n",
    "        self.m_p = m_p\n",
    "        self.m_c = m_c\n",
    "        self.original_m_b = copy.deepcopy(m_b)\n",
    "        self.original_m_p = copy.deepcopy(m_p)\n",
    "        self.original_m_c = copy.deepcopy(m_c)\n",
    "\n",
    "        # self.target_succ_rate = target_succ_rate\n",
    "        # Each action corresponds to choosing to parity checks and the corresponding edges to swap\n",
    "        self.action_space = spaces.MultiDiscrete([\n",
    "            3,  # select which matrix to operate on, m_b, m_p, or m_c\n",
    "            # select which parity check to operate on\n",
    "            params['n_data_qubits'],\n",
    "            # higher than the check qubit index return a low reward\n",
    "            params['n_check_qubits'],\n",
    "            # select which data qubit to operate on. If m_c is selected, have choosing a data qubit\n",
    "        ])\n",
    "        self.last_fer = 0\n",
    "        self.n_steps = 0\n",
    "        self.current_run_len = 0\n",
    "\n",
    "        self.n_qubits = n_qubits = (params['n_data_qubits'] +\n",
    "                                    params['n_check_qubits']) * 2\n",
    "\n",
    "        # The first n qubits represent the noise distribution\n",
    "        # TODO: THIS ALLOWS US TO TRAIN FOR \"ADAPTIVE NOISE!!\" (i.e. lets decrease connections...)\n",
    "        # The quantum parity check matrix\n",
    "        self.observation_space = spaces.Box(low=0, high=1,\n",
    "                                            shape=(params['n_check_qubits'], n_qubits), dtype=np.uint8)\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        p_fails = np.ones(self.n_qubits) * np.random.uniform(\n",
    "            low=params['constant_error_rate_lower'], high=params['constant_error_rate_upper'])\n",
    "        self.last_wsr = self.best_wsr = score_dataset.run_decoder(code_pc, p_fails)\n",
    "\n",
    "        # TODO: WHAT NUMBER???\n",
    "        self.max_run_len = 150\n",
    "        self.reward_step = 0.002\n",
    "\n",
    "    def step(self, action):\n",
    "        p_fails = np.ones(self.n_qubits) * np.random.uniform(\n",
    "            low=params['constant_error_rate_lower'], high=params['constant_error_rate_upper'])\n",
    "        if action[0] == 0:\n",
    "            self.m_b[action[1], action[2]] = 1 - self.m_b[action[1], action[2]]\n",
    "        elif action[0] == 1:\n",
    "            self.m_p[action[1], action[2]] = 1 - self.m_p[action[1], action[2]]\n",
    "        elif action[0] == 2:\n",
    "            if action[1] >= params['n_check_qubits']:\n",
    "                old_code_pc = cpc_code.get_classical_code_cpc(\n",
    "                    self.m_b, self.m_p, self.m_c)\n",
    "                obs = old_code_pc\n",
    "                return obs, -1, False, {}  # Return a very low reward\n",
    "            self.m_c[action[1], action[2]] = 1 - self.m_c[action[1], action[2]]\n",
    "        else:\n",
    "            raise \"Undefined selector action\"\n",
    "\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        # TODO: p_fail??\n",
    "        succ_rate = score_dataset.run_decoder(code_pc, p_fails)\n",
    "\n",
    "        self.last_fer = 1 - succ_rate\n",
    "\n",
    "        # TODO: scaling?\n",
    "        reward = 1 if succ_rate > self.best_wsr + self.reward_step else 0\n",
    "        if reward:\n",
    "            self.best_wsr = succ_rate\n",
    "        self.last_wsr = succ_rate\n",
    "        obs = code_pc\n",
    "\n",
    "        # Update global parameters\n",
    "        self.n_steps += 1\n",
    "\n",
    "        done = succ_rate >= self.target_succ_rate\n",
    "        if self.current_run_len > self.max_run_len:\n",
    "            done = True\n",
    "            self.current_run_len = 0\n",
    "\n",
    "        self.current_run_len += 1\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.m_b = copy.deepcopy(self.original_m_b)\n",
    "        self.m_p = copy.deepcopy(self.original_m_p)\n",
    "        self.m_c = copy.deepcopy(self.original_m_c)\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        return code_pc\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "env = SwapLDPCEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:190: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "# from stable_baselines3.common import set_random_seed, make_vec_env\n",
    "\n",
    "model_type = \"PPO\"\n",
    "check_env(env, warn=True)\n",
    "tf_logs = f\"./log\"\n",
    "\n",
    "def make_env(rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = SwapLDPCEnv() \n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    # set_global_seeds(seed)\n",
    "    return _init\n",
    "\n",
    "num_cpu = 4\n",
    "\n",
    "# wrap it\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])\n",
    "# env = DummyVecEnv([make_env(0)])\n",
    "# make_vec_env(lambda: env,   n_envs=1)\n",
    "\n",
    "loading_saved = False\n",
    "\n",
    "model = None\n",
    "if not loading_saved:\n",
    "\tmodel = PPO(\"MlpPolicy\", env=env, tensorboard_log=tf_logs, verbose=1)\n",
    "else:\n",
    "\t# TODO!\n",
    "\tmodel = PPO.load(utils.get_most_recent_model_path_rl(), env=env, print_system_info=True, tensorboard_log=tf_logs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the callbacks\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, BaseCallback\n",
    "import numpy as np\n",
    "import json\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat, Logger\n",
    "\n",
    "\n",
    "class SummaryWriterCallback(BaseCallback):\n",
    "    '''\n",
    "    Snippet skeleton from Stable baselines3 documentation here:\n",
    "    https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html#directly-accessing-the-summary-writer\n",
    "    '''\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self._log_freq = 1  # log every 10 calls\n",
    "\n",
    "        output_formats = self.logger.output_formats\n",
    "        # Save reference to tensorboard formatter object\n",
    "        # note: the failure case (not formatter found) is not handled here, should be done with try/except.\n",
    "        self.tb_formatter = next(formatter for formatter in output_formats if isinstance(\n",
    "            formatter, TensorBoardOutputFormat))\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        '''\n",
    "        Log my_custom_reward every _log_freq(th) to tensorboard for each environment\n",
    "        '''\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            # rewards = self.locals['my_custom_info_dict']['my_custom_reward']\n",
    "            fers = self.model.get_env().get_attr(\"last_fer\", list(range(num_cpu)))\n",
    "            for i, fer in enumerate(fers):\n",
    "                # TODO: for each env?\n",
    "                self.tb_formatter.writer.add_scalar(\"wer/env #{}\".format(i + 1),\n",
    "                                                    fer,\n",
    "                                                    self.n_calls)\n",
    "            tel = self.logger.name_to_value[\"train/entropy_loss\"]\n",
    "            if tel is not None and tel != 0:\n",
    "                self.tb_formatter.writer.add_scalar(\"train/entropy_loss\",\n",
    "                    tel, self.n_calls)\n",
    "            pgl = self.logger.name_to_value[\"train/policy_gradient_loss\"]\n",
    "            if pgl is not None and pgl != 0:\n",
    "                self.tb_formatter.writer.add_scalar(\"train/policy_gradient_loss\",\n",
    "                    pgl, self.n_calls)\n",
    "            tvl = self.logger.name_to_value[\"train/value_loss\"] \n",
    "            if tvl is not None and tvl != 0:\n",
    "                self.tb_formatter.writer.add_scalar(\"train/value_loss\",\n",
    "                    tvl, self.n_calls)\n",
    "            akl = self.logger.name_to_value[\"train/approx_kl\"]\n",
    "            if akl is not None and akl != 0:\n",
    "                self.tb_formatter.writer.add_scalar(\"train/approx_kl\",\n",
    "                    akl, self.n_calls)\n",
    "        return True\n",
    "\n",
    "# From https://stable-baselines.readthedocs.io/en/master/guide/examples.html\n",
    "class SaveModelOnTraining(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super(SaveModelOnTraining, self).__init__(verbose)\n",
    "        self.check_freq = params['rl_save_model_freq']\n",
    "        self.save_path = utils.get_most_recent_model_path_rl()\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            self.model.save(self.save_path)\n",
    "            print(\"Saving new model to {} for step {}\".format(\n",
    "                self.save_path), self.n_calls)\n",
    "            with open(utils.get_most_recent_model_path_rl_info(), 'w') as f:\n",
    "                data = {\n",
    "                    \"n_steps\": self.n_calls,\n",
    "                    # \"last_fer\":\n",
    "                }\n",
    "                json.dump(data, f)\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./log/PPO_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "            0.0020 ||    34556 |     1735 |     1004 | 6.12e-04 | 2.91e-02 ||    5.314 | 00h00'00  *\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 8    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 926  |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1965        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020166982 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.51       |\n",
      "|    explained_variance   | 0.0339      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    value_loss           | 0.361       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3003        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023346707 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.48       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0818      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 0.313       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4053        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026453784 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.45       |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0397      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 0.257       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "            0.0020 ||    21955 |     1229 |      638 | 6.83e-04 | 2.91e-02 ||    3.323 | 00h00'00  *\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5105        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027567368 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.39       |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0291      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 6163        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029582461 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.33       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0608     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 7163        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032068357 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.28       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0831     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n",
      "(WW) Found in Alist file connections of degree 0!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lev/code/research/ai/AI_QEC/Notebook_RL_Code_Finder.ipynb Cell 12\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lev/code/research/ai/AI_QEC/Notebook_RL_Code_Finder.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m75_000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mSummaryWriterCallback(), progress_bar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPPO\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    308\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    309\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    310\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    311\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    312\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    313\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    244\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    246\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 248\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:175\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    173\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 175\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    179\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:121\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 121\u001b[0m     results \u001b[39m=\u001b[39m [remote\u001b[39m.\u001b[39mrecv() \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes]\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwaiting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     obs, rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:121\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 121\u001b[0m     results \u001b[39m=\u001b[39m [remote\u001b[39m.\u001b[39;49mrecv() \u001b[39mfor\u001b[39;00m remote \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremotes]\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwaiting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     obs, rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mresults)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m    251\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(buf\u001b[39m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    415\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[39m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    380\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=15_000, callback=SummaryWriterCallback(), progress_bar=False, tb_log_name=\"PPO\")\n",
    "# for i in range(25):\n",
    "# \tmodel.learn(total_timesteps=100_000, callback=callback_list)\n",
    "# \tmodel.save(utils.get_most_recent_model_path_rl())\n",
    "# TODO: CHECK OUT ALIST with deg 0 issue??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(utils.get_most_recent_model_path_rl())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
