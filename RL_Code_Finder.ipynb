{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable_baselines3[extra] -q\n",
    "!pip install pyglet==1.5.27 -q\n",
    "!pip install -U bposd -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the common library for CPC codes\n",
    "import os\n",
    "import sys\n",
    "# TODO: lets do something better here like refactor the common parts and different learning mech parts\n",
    "sys.path.append(os.getcwd() + \"/src\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RL Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflatten\u001b[39m(l):\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m [item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m l \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist]\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from global_params import params\n",
    "from scoring import score_dataset\n",
    "from CPC import cpc_code\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Some quick thoughts:\n",
    "-- Should we start with a specific code each time or always a new random code?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SwapLDPCEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, save_best=True):\n",
    "        super(SwapLDPCEnv, self).__init__()\n",
    "\n",
    "        # self.target_succ_rate = target_succ_rate\n",
    "        # Each action corresponds to choosing to parity checks and the corresponding edges to swap\n",
    "        self.action_space = spaces.MultiDiscrete([\n",
    "            3,  # select which matrix to operate on, m_b, m_p, or m_c\n",
    "            # select which parity check to operate on\n",
    "            params['n_check_qubits'],\n",
    "            # select which data qubit to operate on. If m_c is selected, have choosing a data qubit\n",
    "            params['n_data_qubits'],\n",
    "            # higher than the check qubit index return a low reward\n",
    "        ])\n",
    "        self.n_steps = 0\n",
    "        self.save_best = save_best\n",
    "        self.last_save_diff = 0\n",
    "\n",
    "        self.n_qubits = n_qubits = params['n_data_qubits'] + params['n_check_qubits']\n",
    "        flattened_pc_size = 2 * \\\n",
    "            (n_qubits) * \\\n",
    "            params['n_check_qubits']\n",
    "\n",
    "\t\t# The first n qubits represent the noise distribution\n",
    "        # TODO: THIS ALLOWS US TO TRAIN FOR \"ADAPTIVE NOISE!!\" (i.e. lets decrease connections...)\n",
    "        # The quantum parity check matrix\n",
    "        self.observation_space = spaces.Box(low=0, high=self.n - 1,\n",
    "                                            shape=(n_qubits + flattened_pc_size), dtype=np.float32)\n",
    "        self.best_succ = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        self.n_steps += 1\n",
    "        if action[0] == 0:\n",
    "            self.m_b[action[1], action[2]] = 1 - self.m_b[action[1], action[2]]\n",
    "        elif action[0] == 1:\n",
    "            self.m_p[action[1], action[2]] = 1 - self.m_p[action[1], action[2]]\n",
    "        elif action[0] == 2:\n",
    "            if action[2] >= params['n_check_qubits']:\n",
    "                flattened = np.array(self.code_pc_adj).astype(\n",
    "                    np.int16).flatten()\n",
    "                return flattened, -10, False, {}  # Return a very low reward\n",
    "            self.m_c[action[1], action[2]] = 1 - self.m_c[action[1], action[2]]\n",
    "        else:\n",
    "            raise \"Undefined selector action\"\n",
    "\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        # TODO: p_fail??\n",
    "        p_fails = np.ones(self.n_qubits) * np.random.uniform(low=params['constant_error_rate_lower'], high=params['constant_error_rate_upper'])\n",
    "        succ_rate = score_dataset.run_decoder(code_pc, p_fail=self.p_fail)\n",
    "\n",
    "        reward = 0\n",
    "        if self.save_best and succ_rate > self.best_succ and self.last_save_diff > 100:\n",
    "            path = utils.get_best_scoring_model_path_rl()\n",
    "            old_file_path = f\"pcs/pc_{self.best_succ}\"\n",
    "            if self.best_succ != 0 and os.path.isfile(old_file_path):\n",
    "                os.remove(old_file_path)\n",
    "            print(\"Saving new best with succ_rate\", succ_rate)\n",
    "            filename_base = f\"pc-[{self.n},{self.k}]-p{self.p_fail}\"\n",
    "            np.savetxt(f\"pcs/{filename_base}-pc.txt\", self.code_pc)\n",
    "            f = open(f\"pcs/{filename_base}-data.txt\", \"w\")\n",
    "            self.n_samples_no_change_max_runs_accum = 0\n",
    "            self.last_save_diff = 0\n",
    "        else:\n",
    "            self.last_save_diff += 1\n",
    "\n",
    "        flattened = np.array(code_pc).astype(np.float32).flatten()\n",
    "        obs = np.concatenate(p_fails, flatten)\n",
    "        return flattened, reward, succ_rate >= self.target_succ_rate, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.code_pc = self.start_pc\n",
    "        self.code_pc_adj = self.start_pc_adj\n",
    "        # reward, done, info can't be included\n",
    "        npd = np.array(self.code_pc_adj).astype(np.int16)\n",
    "        return npd.flatten()\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "n = GLOBAL_N\n",
    "k = GLOBAL_K\n",
    "degRowAvg = GLOBAL_DEG_ROW\n",
    "# TODO: this gen_random_ldpc is NO GOOD FOR NOW: it starts with a random number of edges and because we never add or subract edges for now\n",
    "# This poses a problem. TODO: there may be something smart to do here later with adding subbing edges\n",
    "\n",
    "\n",
    "def mat_to_adj_list(H):\n",
    "    def row_adj(i):\n",
    "        return [j for j in range(H[i].shape[-1]) if H[i][j] > 0]\n",
    "    adj_list = [row_adj(i) for i in range(H.shape[0])]\n",
    "    return adj_list\n",
    "\n",
    "# A fundamental problem we have here is the speed at which we determine what a \"good\" code is\n",
    "# Maybe there is a faster way... this is where something like literature review would be necessary\n",
    "# Alternatively, we an increase samples_per_step once the difference in performance is no longer visible\n",
    "# Also an important lever here is p_fail. I.e. we may want it to be higher depending on certain factors (i.e. as codes get better\n",
    "# we almost want p to be \"adaptive\")\n",
    "\n",
    "\n",
    "H, G = gen_random_ldpc(n, k, deg_row=degRowAvg)\n",
    "env = SwapLDPCEnv(H, mat_to_adj_list(H), degRowAvg,\n",
    "                  samples_per_step=1_000, p_fail=0.1)\n",
    "check_env(env, warn=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
