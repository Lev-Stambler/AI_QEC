{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable_baselines3[extra] -q\n",
    "!pip install pyglet==1.5.27 -q\n",
    "!pip install -U bposd -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the common library for CPC codes\n",
    "import os\n",
    "import sys\n",
    "# TODO: lets do something better here like refactor the common parts and different learning mech parts\n",
    "sys.path.append(os.getcwd() + \"/src\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RL Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from global_params import params\n",
    "from scoring import score_dataset\n",
    "from CPC import cpc_code, generate_random as gen_random_cpc\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Some quick thoughts:\n",
    "-- Should we start with a specific code each time or always a new random code?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SwapLDPCEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, target_succ_rate=0.99):\n",
    "        super(SwapLDPCEnv, self).__init__()\n",
    "\n",
    "        self.target_succ_rate = target_succ_rate\n",
    "        _, m_b, m_p, m_c = gen_random_cpc.random_cpc()\n",
    "        self.m_b = m_b\n",
    "        self.m_p = m_p\n",
    "        self.m_c = m_c\n",
    "        # self.target_succ_rate = target_succ_rate\n",
    "        # Each action corresponds to choosing to parity checks and the corresponding edges to swap\n",
    "        self.action_space = spaces.MultiDiscrete([\n",
    "            3,  # select which matrix to operate on, m_b, m_p, or m_c\n",
    "            # select which parity check to operate on\n",
    "            params['n_data_qubits'],\n",
    "            # higher than the check qubit index return a low reward\n",
    "            params['n_check_qubits'],\n",
    "            # select which data qubit to operate on. If m_c is selected, have choosing a data qubit\n",
    "        ])\n",
    "        self.last_fer = 0\n",
    "        self.n_steps = 0\n",
    "\n",
    "        self.n_qubits = n_qubits = params['n_data_qubits'] + \\\n",
    "            params['n_check_qubits']\n",
    "        flattened_pc_size = 2 * \\\n",
    "            (n_qubits) * \\\n",
    "            params['n_check_qubits']\n",
    "\n",
    "        # The first n qubits represent the noise distribution\n",
    "        # TODO: THIS ALLOWS US TO TRAIN FOR \"ADAPTIVE NOISE!!\" (i.e. lets decrease connections...)\n",
    "        # The quantum parity check matrix\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0,\n",
    "                                            shape=(n_qubits + flattened_pc_size,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        p_fails = np.ones(self.n_qubits) * np.random.uniform(\n",
    "            low=params['constant_error_rate_lower'], high=params['constant_error_rate_upper'])\n",
    "        if action[0] == 0:\n",
    "            self.m_b[action[1], action[2]] = 1 - self.m_b[action[1], action[2]]\n",
    "        elif action[0] == 1:\n",
    "            self.m_p[action[1], action[2]] = 1 - self.m_p[action[1], action[2]]\n",
    "        elif action[0] == 2:\n",
    "            if action[1] >= params['n_check_qubits']:\n",
    "                old_code_pc = cpc_code.get_classical_code_cpc(\n",
    "                    self.m_b, self.m_p, self.m_c)\n",
    "                flattened = np.array(old_code_pc).astype(np.float32).flatten()\n",
    "                obs = np.concatenate((p_fails, flattened)).astype(np.float32)\n",
    "                return obs, -1, False, {}  # Return a very low reward\n",
    "            self.m_c[action[1], action[2]] = 1 - self.m_c[action[1], action[2]]\n",
    "        else:\n",
    "            raise \"Undefined selector action\"\n",
    "\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        # TODO: p_fail??\n",
    "        succ_rate = score_dataset.run_decoder(code_pc, p_fails)\n",
    "\n",
    "        self.last_fer = 1 - succ_rate\n",
    "\n",
    "        # TODO: scaling?\n",
    "        reward = succ_rate\n",
    "        flattened = np.array(code_pc).astype(np.float32).flatten()\n",
    "        obs = np.concatenate((p_fails, flattened)).astype(np.float32)\n",
    "\n",
    "\t\t# Update global parameters\n",
    "        self.n_steps += 1\n",
    "\n",
    "        return obs, reward, succ_rate >= self.target_succ_rate, {}\n",
    "\n",
    "    def reset(self):\n",
    "        p_fails = np.ones(self.n_qubits) * np.random.uniform(\n",
    "            low=params['constant_error_rate_lower'], high=params['constant_error_rate_upper'])\n",
    "        _, m_b, m_p, m_c = gen_random_cpc.random_cpc()\n",
    "        self.m_b = m_b\n",
    "        self.m_p = m_p\n",
    "        self.m_c = m_c\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        # reward, done, info can't be included\n",
    "        npd = np.array(code_pc).astype(np.float32)\n",
    "        return np.concatenate((p_fails, npd.flatten())).astype(np.float32)\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "env = SwapLDPCEnv()\n",
    "check_env(env, warn=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = SwapLDPCEnv() \n",
    "model_type = \"PPO\"\n",
    "check_env(env, warn=True)\n",
    "tf_logs = \"./logs/{model_type}-tensorboard\"\n",
    "\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env,   n_envs=1)\n",
    "loading_saved = False\n",
    "model = None\n",
    "if not loading_saved:\n",
    "\tmodel = PPO(\"MlpPolicy\", env=env, tensorboard_log=tf_logs)\n",
    "else:\n",
    "\t# TODO!\n",
    "\tmodel = PPO.load(utils.get_best_scoring_model_path_rl(), env=env, print_system_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the callbacks\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, BaseCallback\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        self.is_tb_set = False\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        writer = tf.summary.create_file_writer(tf_logs) ## TODO?\n",
    "        self.writer = writer\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log additional tensor\n",
    "        # if not self.is_tb_set:\n",
    "        #     with self.model.graph.as_default():\n",
    "        #         tf.summary.scalar('value_target', tf.reduce_mean(self.model.value_target))\n",
    "        #         self.model.summary = tf.summary.merge_all()\n",
    "        #     self.is_tb_set = True\n",
    "\n",
    "        # Log scalar value (here a random variable)\n",
    "        env = self.model.get_env().envs[0]\n",
    "        fer = env.last_fer\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('Frame Error Rate', fer, step=env.n_steps)\n",
    "            self.writer.flush()\n",
    "        return True\n",
    "\n",
    "\n",
    "# From https://stable-baselines.readthedocs.io/en/master/guide/examples.html\n",
    "class SaveModelOnTraining(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(SaveModelOnTraining, self).__init__(verbose)\n",
    "        self.check_freq = params['rl_save_model_freq']\n",
    "        self.save_path = utils.get_most_recent_model_path_rl()\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "          \n",
    "          self.model.save(self.save_path)\n",
    "          print(\"Saving new model to {} for step {}\".format(self.save_path), self.n_calls)\n",
    "          with open(utils.get_most_recent_model_path_rl_info(), 'w') as f:\n",
    "            data = {\n",
    "              \"n_steps\": self.n_calls,\n",
    "              # \"last_fer\": \n",
    "            }\n",
    "            json.dump(data, f)\n",
    "        return True\n",
    "\n",
    "\n",
    "callback_list = CallbackList([TensorboardCallback(), SaveModelOnTraining()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:273: UserWarning: Path 'best_scoring/best_model_rl_bsc_iid_noise_(31,11).zip' is a folder. Will save instead to best_scoring/best_model_rl_bsc_iid_noise_(31,11).zip_2\n",
      "  warnings.warn(f\"Path '{path}' is a folder. Will save instead to {path}_2\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Replacement index 1 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m250_000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback_list)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39m: PPOSelf,\n\u001b[1;32m    305\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PPOSelf:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    318\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    319\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    320\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    321\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    322\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    323\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    324\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    325\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    326\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    327\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 262\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:187\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    186\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m--> 187\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:100\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:204\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m continue_training \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    203\u001b[0m     \u001b[39m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     continue_training \u001b[39m=\u001b[39m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mand\u001b[39;00m continue_training\n\u001b[1;32m    205\u001b[0m \u001b[39mreturn\u001b[39;00m continue_training\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:100\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "Cell \u001b[0;32mIn [14], line 61\u001b[0m, in \u001b[0;36mSaveModelOnTraining._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_path)\n\u001b[0;32m---> 61\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39mSaving new model to \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m for step \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_path), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls)\n\u001b[1;32m     62\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(utils\u001b[39m.\u001b[39mget_most_recent_model_path_rl_info(), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     63\u001b[0m     data \u001b[39m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mn_steps\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls,\n\u001b[1;32m     65\u001b[0m       \u001b[39m# \"last_fer\": \u001b[39;00m\n\u001b[1;32m     66\u001b[0m     }\n",
      "\u001b[0;31mIndexError\u001b[0m: Replacement index 1 out of range for positional args tuple"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=250_000, callback=callback_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
