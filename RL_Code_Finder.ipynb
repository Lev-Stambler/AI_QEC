{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable_baselines3[extra] -q\n",
    "!pip install pyglet==1.5.27 -q\n",
    "!pip install -U bposd -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the common library for CPC codes\n",
    "import os\n",
    "import sys\n",
    "# TODO: lets do something better here like refactor the common parts and different learning mech parts\n",
    "sys.path.append(os.getcwd() + \"/src\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RL Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from global_params import params\n",
    "from scoring import score_dataset\n",
    "from CPC import cpc_code, generate_random as gen_random_cpc\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Some quick thoughts:\n",
    "-- Should we start with a specific code each time or always a new random code?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SwapLDPCEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, target_succ_rate=0.99):\n",
    "        super(SwapLDPCEnv, self).__init__()\n",
    "\n",
    "        self.target_succ_rate = target_succ_rate\n",
    "        _, m_b, m_p, m_c = gen_random_cpc.random_cpc()\n",
    "        self.m_b = m_b\n",
    "        self.m_p = m_p\n",
    "        self.m_c = m_c\n",
    "        # self.target_succ_rate = target_succ_rate\n",
    "        # Each action corresponds to choosing to parity checks and the corresponding edges to swap\n",
    "        self.action_space = spaces.MultiDiscrete([\n",
    "            3,  # select which matrix to operate on, m_b, m_p, or m_c\n",
    "            # select which parity check to operate on\n",
    "            params['n_data_qubits'],\n",
    "            # higher than the check qubit index return a low reward\n",
    "            params['n_check_qubits'],\n",
    "            # select which data qubit to operate on. If m_c is selected, have choosing a data qubit\n",
    "        ])\n",
    "        self.last_fer = 0\n",
    "        self.n_steps = 0\n",
    "\n",
    "        self.n_qubits = n_qubits = params['n_data_qubits'] + \\\n",
    "            params['n_check_qubits']\n",
    "        flattened_pc_size = 2 * \\\n",
    "            (n_qubits) * \\\n",
    "            params['n_check_qubits']\n",
    "\n",
    "        # The first n qubits represent the noise distribution\n",
    "        # TODO: THIS ALLOWS US TO TRAIN FOR \"ADAPTIVE NOISE!!\" (i.e. lets decrease connections...)\n",
    "        # The quantum parity check matrix\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0,\n",
    "                                            shape=(n_qubits + flattened_pc_size,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        p_fails = np.ones(self.n_qubits) * np.random.uniform(\n",
    "            low=params['constant_error_rate_lower'], high=params['constant_error_rate_upper'])\n",
    "        if action[0] == 0:\n",
    "            self.m_b[action[1], action[2]] = 1 - self.m_b[action[1], action[2]]\n",
    "        elif action[0] == 1:\n",
    "            self.m_p[action[1], action[2]] = 1 - self.m_p[action[1], action[2]]\n",
    "        elif action[0] == 2:\n",
    "            if action[1] >= params['n_check_qubits']:\n",
    "                old_code_pc = cpc_code.get_classical_code_cpc(\n",
    "                    self.m_b, self.m_p, self.m_c)\n",
    "                flattened = np.array(old_code_pc).astype(np.float32).flatten()\n",
    "                obs = np.concatenate((p_fails, flattened)).astype(np.float32)\n",
    "                return obs, -1, False, {}  # Return a very low reward\n",
    "            self.m_c[action[1], action[2]] = 1 - self.m_c[action[1], action[2]]\n",
    "        else:\n",
    "            raise \"Undefined selector action\"\n",
    "\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        # TODO: p_fail??\n",
    "        succ_rate = score_dataset.run_decoder(code_pc, p_fails)\n",
    "\n",
    "        self.last_fer = 1 - succ_rate\n",
    "\n",
    "        # TODO: scaling?\n",
    "        reward = succ_rate\n",
    "        flattened = np.array(code_pc).astype(np.float32).flatten()\n",
    "        obs = np.concatenate((p_fails, flattened)).astype(np.float32)\n",
    "\n",
    "\t\t# Update global parameters\n",
    "        self.n_steps += 1\n",
    "\n",
    "        return obs, reward, succ_rate >= self.target_succ_rate, {}\n",
    "\n",
    "    def reset(self):\n",
    "        p_fails = np.ones(self.n_qubits) * np.random.uniform(\n",
    "            low=params['constant_error_rate_lower'], high=params['constant_error_rate_upper'])\n",
    "        _, m_b, m_p, m_c = gen_random_cpc.random_cpc()\n",
    "        self.m_b = m_b\n",
    "        self.m_p = m_p\n",
    "        self.m_c = m_c\n",
    "        code_pc = cpc_code.get_classical_code_cpc(self.m_b, self.m_p, self.m_c)\n",
    "        # reward, done, info can't be included\n",
    "        npd = np.array(code_pc).astype(np.float32)\n",
    "        return np.concatenate((p_fails, npd.flatten())).astype(np.float32)\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "env = SwapLDPCEnv()\n",
    "check_env(env, warn=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = SwapLDPCEnv() \n",
    "model_type = \"PPO\"\n",
    "check_env(env, warn=True)\n",
    "tf_logs = \"./logs/{model_type}-tensorboard\"\n",
    "\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env,   n_envs=1)\n",
    "loading_saved = False\n",
    "model = None\n",
    "if not loading_saved:\n",
    "\tmodel = PPO(\"MlpPolicy\", env=env, tensorboard_log=tf_logs)\n",
    "else:\n",
    "\t# TODO!\n",
    "\tmodel = PPO.load(utils.get_best_scoring_model_path_rl(), env=env, print_system_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the callbacks\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, BaseCallback\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        self.is_tb_set = False\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        writer = tf.summary.create_file_writer(tf_logs) ## TODO?\n",
    "        self.writer = writer\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log additional tensor\n",
    "        # if not self.is_tb_set:\n",
    "        #     with self.model.graph.as_default():\n",
    "        #         tf.summary.scalar('value_target', tf.reduce_mean(self.model.value_target))\n",
    "        #         self.model.summary = tf.summary.merge_all()\n",
    "        #     self.is_tb_set = True\n",
    "\n",
    "        # Log scalar value (here a random variable)\n",
    "        env = self.model.get_env().envs[0]\n",
    "        fer = env.last_fer\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('Frame Error Rate', fer, step=env.n_steps)\n",
    "            self.writer.flush()\n",
    "        return True\n",
    "\n",
    "\n",
    "# From https://stable-baselines.readthedocs.io/en/master/guide/examples.html\n",
    "class SaveModelOnTraining(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(SaveModelOnTraining, self).__init__(verbose)\n",
    "        self.check_freq = params['rl_save_model_freq']\n",
    "        self.save_path = utils.get_most_recent_model_path_rl()\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "          \n",
    "          self.model.save(self.save_path)\n",
    "          print(\"Saving new model to {} for step {}\".format(self.save_path), self.n_calls)\n",
    "          with open(utils.get_most_recent_model_path_rl_info(), 'w') as f:\n",
    "            data = {\n",
    "              \"n_steps\": self.n_calls,\n",
    "              # \"last_fer\": \n",
    "            }\n",
    "            json.dump(data, f)\n",
    "        return True\n",
    "\n",
    "\n",
    "callback_list = CallbackList([TensorboardCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "\tmodel.learn(total_timesteps=10_000, callback=callback_list)\n",
    "\tmodel.save(utils.get_most_recent_model_path_rl())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
